{
  "url": "http://www.stsc.hill.af.mil/crosstalk/2004/11/0411Hunt.html",
  "file_number": "731",
  "content_hash": "46becf4696b477bc9668310a3b4b1c322cdff9b26fb39f7a74b95d47b87668d2",
  "total_blocks": 97,
  "retained_blocks": 31,
  "ignored_blocks": 66,
  "blocks": [
    {
      "tag": "div",
      "text": "html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" Entire Site CrossTalk Only - Mission - Staff - Contact Us - Subscribe Now - Update - Cancel - Home > CrossTalkNov2004 > Article Nov2004Issue Three Essential Tools for Stable Development Andy Hunt,The Pragmatic Programmers, LLC Dave Thomas,The Pragmatic Programmers, LLC Three basic practices make the difference between a software project that succeeds and one that fails. These practices support\r\nand reinforce each other; when done properly, they form an interlocking safety net to help ensure success and prevent common\r\nproject disasters. However, few development teams in the United States use these proven techniques, and even fewer use them\r\ncorrectly."
    },
    {
      "tag": "p",
      "text": "Many software projects that fail seem\r\nto fail for very similar reasons. After\r\nobserving  and helping  many of these\r\nailing projects over the past couple of\r\ndecades, it seems clear to us that a majority\r\nof common problems can be traced\r\nback to a lack of three very basic practices.\r\nFortunately, these three practices are easy\r\nand relatively inexpensive to adopt. It does\r\nnot require a large-scale, expensive, or\r\nbureaucratic effort; with just these practices\r\nin place, your team can work at top\r\nspeed with increased parallelism. You will\r\nnever lose precious work, and you will\r\nknow immediately when the development\r\nstarts to veer off-track in time to correct it,\r\ncheaply and easily. The three basic practices that we have\r\nidentified as being the most crucial are version\r\ncontrol, unit testing, and automation. Version control is an obvious best practice,\r\nyet nearly 40 percent of software projects\r\nin the United States do not use any form\r\nof version control for their source code\r\nfiles [1]. The motto of these shops seems\r\nto be last one in wins . That is, they will use a\r\nshared drive of some sort and hope that\r\nno one overwrites their changes as the\r\nsoftware evolves. Hope is a pretty poor\r\nmethodology, and these teams regularly\r\nlose precious work. Developers begin to\r\nfear making any changes at all, in case they\r\naccidentally make the system worse. Of\r\ncourse, this fear becomes a self-fulfilling\r\nprophecy as necessary changes are neglected\r\nand the system begins to degrade. Unit testing is a coding technique for\r\nprogrammers so they can verify that the\r\ncode they just wrote actually does something\r\nakin to their intent. It may or may\r\nnot fulfill the requirements, but that is a\r\nseparate question: If the code does not do\r\nwhat the programmer thought it did, then\r\nany further testing or validation is both\r\nmeaningless and a large waste of time and\r\nmoney (two items that are in short supply\r\nto begin with). Developer-centric unit\r\ntesting is a great way to introduce basic\r\nregression testing, create more modularized\r\ncode that is easier to maintain, and\r\nensure that new work does not break\r\nexisting work. Despite the effectiveness of\r\nthis technique in both improving design\r\nand identifying and preventing defects\r\n(aka bugs), 76 percent of companies in the\r\nUnited States do not even try it [2]. Automation is a catchall category that\r\nincludes regular, unattended project builds,\r\nincluding regression tests and push-button\r\nconvenience for day-to-day activities.\r\nRegular builds ensure that the product can\r\nbe built to catch simple mistakes early and\r\neasily, when fixing them is the cheapest.\r\nWhen implemented properly, it is as if you\r\nhave an ever-vigilant guardian looking over\r\nyour shoulder, warning you as soon as\r\nthere is a problem. Incredibly, some 70\r\npercent of projects in the United States do\r\nnot have any sort of daily build [2]. By the\r\ntime they discover a problem, it has metastasized\r\ninto a much larger and potentially\r\nfatal problem. We will briefly examine each of these\r\nareas, with an in-depth look at unit testing\r\nin particular. We will outline the important\r\nideas, synergies, and caveats for each of\r\nthese practices so your team can either\r\nbegin using them or improve your current\r\nuse of them."
    },
    {
      "tag": "div",
      "text": "Version Control"
    },
    {
      "tag": "p",
      "text": "Everyone can agree that version control is\r\na best practice but even with it in place, is\r\nit being used effectively? Ask yourself\r\nthese questions: Can you re-create your\r\nsoftware exactly as it existed on January 8?\r\nWhen a bug is found that affects multiple\r\nversions of your released software, can\r\nyour team fix it just once, and then apply\r\nthat fix to the different versions automatically?\r\nCan a developer quickly back out of\r\na bad piece of code? There is more to version control than\r\njust keeping track of files. But before we\r\nproceed, we need to define some simple\r\nterminology: We use check-in to mean that\r\na programmer has submitted his or her\r\nchanges to the version control system. We\r\nuse checkout to refer to getting a personal\r\nversion of code from the version control\r\nsystem into a local working area. When a programmer checks in code, it\r\nis now potentially available to the rest of\r\nthe team. As such, it is only polite to\r\nensure that this new code actually compiles\r\nsuccessfully; it should be accompanied by\r\nunit tests (more on this later), and those\r\ntests should pass. All the other passing\r\ntests in the system should continue to pass\r\nas well  if they suddenly fail, then you can\r\neasily trace the failure to the new code that\r\nwas introduced. It is far easier to track down these sort\r\nof problems right at the point of creation\r\ninstead of days, weeks, or even months\r\nlater. To exploit this effect, you must allow\r\nand encourage frequent check-ins of code\r\nmultiple times per day. It is not unusual to\r\nsee team members check-in code 10-20\r\ntimes a day. It is unusual  and very dangerous\r\n to allow a programmer to go a\r\nfew days or a week or more without checking\r\nin code. Because check-ins occur so frequently,\r\nthese and other day-to-day operations\r\nmust be very fast and low ceremony. Acheck-in or checkout of code should not\r\ntake more than five to 15 seconds in general.\r\nIf it takes an hour, people will not do\r\nit, and you have lost the advantage. Now some people get a little nervous\r\nwhen they read this part. They fret that all\r\nof this code is being dumped into the system\r\nwithout being reviewed, tested by QA,\r\naudited, or whatever else their methodology\r\nor environment demands. They are\r\nrightfully concerned that this code is not\r\nyet ready to be part of a release.\r\nNonetheless, it must still be in the version\r\ncontrol system so that it is protected. Most version control systems provide a\r\nmechanism to differentiate ongoing development\r\nchanges from official release candidates.\r\nSome feature explicit promotion commands to allow this. You can accomplish\r\nthe same thing in other systems by\r\nusing tags (or version labels) to identify\r\nstable release versions of source code as\r\nopposed to code that is in progress. Regardless of the mechanism, it must\r\nbe an easy operation to promote development\r\nchanges to an official release status.\r\nOn the other side of the coin, you need to\r\nbe able to back out changes and any disastrous\r\nnew code when needed. Finally, you need to be able to re-create\r\nany product built at any previous point\r\nin time. This ability to go back in time is\r\ncrucial for effective debugging and problem\r\nsolving (just think of any developer\r\nwho starts a discussion with, \"Well, it used\r\nto work\"). Commercial and freely available version\r\ncontrol systems vary in complexity,\r\nfeatures, and ease of administration. But\r\none feature in particular is worth examining:\r\nwhether it supports strict locking or\r\noptimistic locking. In systems under strict\r\nlocking, only one person can edit a file at\r\na time. While that sounds like a good idea,\r\nit turns out to be unduly restrictive in\r\npractice. We favor the Concurrent\r\nVersion System www.cvshome.org described in [3]. You may find you can increase parallelism\r\nand efficiency in your team by using\r\na system that features optimistic locking.\r\nIn these systems, multiple people can edit\r\nthe same source code file simultaneously.\r\nThe system uses conflict-resolution algorithms\r\nto merge the disparate changes\r\ntogether in a sensible manner. Ninety-nine\r\npercent of the time it works perfectly without\r\nintervention. Occasionally, however,\r\nthere is a conflict that must be addressed\r\nmanually. At no point is anyone's work in\r\ndanger of being lost, and it ends up being\r\nmuch more efficient to coordinate just\r\nthese few conflicts by hand instead of having\r\neveryone coordinate every change with\r\nthe rest of the team."
    },
    {
      "tag": "div",
      "text": "Unit Testing"
    },
    {
      "tag": "p",
      "text": "When a developer makes a change to the\r\ncode on your project, what feedback is\r\navailable? Does the developer have any\r\nway of knowing if the new code broke\r\nanything else? Better still, how do you know\r\nif any developer has broken anything\r\ntoday? Asystem of automated unit tests\r\nwill give you this information in real-time. Programming languages are notorious\r\nfor doing exactly what programmers say,\r\nnot what they mean. Like a petulant child\r\nthat takes your expressions completely literally,\r\nthe computer follows our instructions\r\nto the letter, with no regard at all to\r\nour intent. Technology has yet to produce\r\nthe compiler that implements with do what\r\nImean, not what Isay. So in keeping with the idea of finding\r\nand fixing problems as soon as they occur,\r\nyou want programmers to use unit tests (or\r\nchecked examples) to verify the computer's\r\nliteral interpretation of their commands. It\r\nis really no different from following\r\nthrough with a subordinate to verify that a\r\ndelegated task was performed  except\r\nthat instead of just checking once, automated\r\nunit tests will check and recheck\r\nevery time any code is changed. There are some requirements to using\r\nthis style of development, however: The code base must be decoupled\r\nenough to allow testing. When code is\r\ntightly coupled, it is very difficult to\r\ntest individual pieces in isolation, and\r\nharder to devise unit tests that exercise\r\nspecific areas of functionality. Wellwritten\r\ncode, on the other hand, is easy\r\nto test. If your team finds that the code\r\nis difficult to test, then take that as a\r\nwarning sign that the code is in serious\r\ntrouble to begin with. Only check-in tested code. As we mentioned\r\nabove, checking-in foists a programmer's\r\ncode onto the rest of the\r\nteam. Once it is available to everyone,\r\nthen the whole team will begin to rely\r\non it. Because of this reliance, all code\r\nthat is checked in must pass its own\r\ntests. In addition to passing its own tests, the\r\nprogrammer checking in the code must\r\nensure nothing else breaks, either. This\r\nsimple regression helps prevent that\r\nfrustrating feeling of one step forward, two\r\nsteps back that becomes commonplace\r\nwhen code fixes cause collateral damage\r\nto other parts of the code base.\r\nUsually these bugs then require fixes,\r\nwhich in turn cause more damage, and\r\nso on. The discipline of keeping all the\r\ntests running all the time prevents that\r\nparticular death-spiral. There should be at least as much test\r\ncode as production code. You might\r\nthink that is excessive, but it is really\r\njust a question of where the value of\r\nthe system resides. We firmly believe\r\nthe code that implements the system is\r\nnot where the value of your intellectual\r\nproperty lies. Code can be rewritten\r\nand replaced, and the new code (even\r\nan entirely new system) can be verified\r\nagainst the existing tests. Now the\r\nmost precise specification of the system\r\nis in executable form  the unit\r\ntests. The learning and experience that\r\ngoes into creating the unit tests is\r\ninvaluable, and the tests themselves are\r\nthe best expression we have of that\r\nknowledge. We will look at implementing unit tests\r\n(aka checked examples) in much greater\r\ndetail later in this article."
    },
    {
      "tag": "div",
      "text": "Automation"
    },
    {
      "tag": "p",
      "text": "An old saying goes the cobbler's children have\r\nno shoes . This saying is particularly appropriate\r\nfor our use of software tools during\r\nsoftware development. We see teams routinely\r\nwaste time using manual procedures\r\nthat could easily be automated. Everyone clamors for software development\r\nto be more defined and repeatable.\r\nWell, the design and implementation of\r\nsoftware probably cannot be made repeatable\r\nany more than you could make the\r\nprocess of making hit movies repeatable.\r\nBut the production of software is another\r\nmatter entirely. The process of taking source code\r\nfiles, bits of eXtensible Markup Language,\r\nlibraries, and other resources and producing\r\nan executable for the end user should\r\nbe precisely repeatable. Given the same\r\ninputs, you want the same outputs, every\r\ntime, without excuses. In combination\r\nwith version control, you want to be able\r\nto go back in time and reproduce that\r\nsame pile of bits that you would have produced\r\non January 8 just as easily. That\r\ncomes in very handy should the\r\nDepartment of Justice ask for it politely, or\r\na frustrated customer asks for it somewhat\r\nless politely to work around some outstanding\r\nbug. The rule we try to adopt is that any\r\nmanual process that is repeated twice is\r\nlikely to be repeated a third time  or more\r\n so it needs to be encapsulated within a\r\nshell script, batch file, piece of Java code,\r\nJob Control Language, or whatever. Unit tests, as well as functional and\r\nacceptance tests, should be run automatically\r\nas well as be part of the build process.\r\nYou will probably want to run the unit\r\ntests (which should execute very quickly)\r\nwith every build; automatic functional and\r\nacceptance tests might take longer and you\r\nmay only want to run those once a week,\r\nor when convenient. You see, not only does automation\r\nmake developer's lives easier by providing\r\npush-button convenience, it helps keep the\r\nfeedback coming by constantly checking\r\nthe state of the software. Automated\r\nbuilds are constantly asking two questions:\r\nDoes the software build correctly? Do all\r\nthe tests still pass a basic regression? With\r\nthe computer performing these checks\r\nregularly, developers do not have to.\r\nProblems can be identified as soon as they\r\nhappen, and the appropriate developer or\r\nteam lead can be notified immediately of\r\nthe problem [4]. Problems can be fixed\r\nquickly, before they have a chance to cause\r\nany additional damage. That is the benefit\r\nwe want from automation. Finally, consider how the build communicates\r\nto the development team and its\r\nmanagement. Does the team lead look at\r\nthe latest results in some log file and then\r\nreport status to management? Does not\r\nthat constitute a manual process? It is relatively\r\neasy to set up visual display devices,\r\nranging from liquid crystal display screens\r\nto bubbling lava-style lamps to the new\r\nand popular Ambient Orb [4]."
    },
    {
      "tag": "div",
      "text": "Synergy"
    },
    {
      "tag": "p",
      "text": "These three practices interlock to provide a\r\ngenuine safety net for developers. Version\r\ncontrol is the foundation. Unit tests and\r\nscripts for automation are under version\r\ncontrol, but version control needs automation\r\nto be effective. Unit testing needs both\r\nversion control and automation. With the combination, developers can\r\nbetter afford to take chances, experiment,\r\nand find the best solutions. The Rule of\r\nThree says that if you have not proposed\r\nat least three solutions to a problem then\r\nyou have not thought about it hard\r\nenough. With this set of practices in place,\r\ndevelopers can realistically try out a number\r\nof different solutions to a problem:\r\nVersion control will keep them separate,\r\nand unit testing will help confirm the viability\r\nof each solution. All this with plenty\r\nof automated support, including continuous,\r\nongoing checks ensures that the team\r\ndoes not wander too far off into the\r\nwoods. This is how modern, successful\r\nsoftware development is done."
    },
    {
      "tag": "div",
      "text": "Unit Testing With Your\r\nRight-BICEP"
    },
    {
      "tag": "p",
      "text": "You can strengthen your organization's\r\ntesting skills by looking at six specific areas\r\nof code that may need unit tests. These\r\nareas are remembered easily using the\r\nmnemonic Right-BICEP [5]: Right Are the results right? B Are all the boundary conditions correct? I Can you check inverse relationships? C Can you cross-check results using\r\nother means? E Can you force error conditions to\r\nhappen? P Are performance characteristics\r\nwithin bounds?"
    },
    {
      "tag": "div",
      "text": "Are the Results Right?"
    },
    {
      "tag": "p",
      "text": "The first and most obvious area to test is\r\nsimply to see if the expected results are\r\nright  to validate the results. These are\r\nusually the easy tests, as they represent the\r\nanswer to the key question: If the code ran\r\ncorrectly, how would Iknow? Here is an\r\nexample of how being forced to think\r\nabout testing helps developers code better:\r\nIf this question cannot be answered satisfactorily,\r\nthen writing the code  or the test\r\n may be a complete waste of time. \"But wait,\" you cry out, \"that does not\r\nsound very agile! What if the requirements\r\nare vague or incomplete? Does that mean\r\nwe can't write code until all the requirements\r\nare firm?\" No, it does not at all. If\r\nthe requirements are truly not yet known,\r\nor not yet complete, you can always make\r\nsome assumptions as a stake in the ground.\r\nThey may not be correct from the user's\r\npoint of view (or anyone else on the planet),\r\nbut they let the team continue to develop.\r\nAnd, because you have written a test\r\nbased on your assumption, you have now\r\ndocumented it  nothing is implicit. Of course, you must then arrange for\r\nfeedback with users or sponsors to finetune\r\nyour assumptions. The definition of correct may change over the lifetime of the\r\ncode in question, but at any point, you\r\nshould be able to prove that it is doing\r\nwhat you think it ought."
    },
    {
      "tag": "div",
      "text": "Boundary Conditions"
    },
    {
      "tag": "p",
      "text": "Identifying boundary conditions is one of\r\nthe most valuable parts of unit testing\r\nbecause this is where most bugs generally\r\nlive  at the edges. Some conditions you\r\nmight want to think about include the following: Totally bogus or inconsistent input values\r\nsuch as a file name of\r\n!*W:X\\\\{\\Gi/w$$g/h\\#WQ@. Badly formatted data such as an e-mail\r\naddress without a top-level domain\r\nfred@foobar. Empty or missing values such as 0, 0.0,\r\n\"\", or null. Values far in excess of reasonable\r\nexpectations such as a person's age of\r\n10,000 years. Duplicates in lists that should not have\r\nduplicates. Ordered lists that are not in order and\r\nvice-versa. Try handing a pre-sorted list\r\nto a sort algorithm, for instance, or\r\neven a reverse-sorted list. Things that arrive out of order, or happen\r\nout of expected order such as trying\r\nto print a document before logging\r\nin, for instance. An easy way to think of possible\r\nboundary conditions is to remember the\r\nacronym CORRECT. For each of these\r\nitems, consider whether or not similar conditions\r\nmay exist in your method that you\r\nwant to test, and what might happen if\r\nthese conditions were violated [4]: Conformance. Does the value conform\r\nto an expected format? Ordering. Is the set of values ordered\r\nor unordered as appropriate? Range. Is the value within reasonable\r\nminimum and maximum values? Reference. Does the code reference\r\nanything external that is not under\r\ndirect control of the code itself ? Existence. Does the value exist (e.g., is\r\nnon-null, non-zero, present in a set,\r\netc.)? Cardinality. Are there exactly enough\r\nvalues? Time (absolute and relative). Is\r\neverything happening in order? At the\r\nright time? In time?"
    },
    {
      "tag": "div",
      "text": "Check Inverse Relationships"
    },
    {
      "tag": "p",
      "text": "Some methods can be checked by applying\r\ntheir logical inverse. For instance developers\r\nmight check a method that calculates a\r\nsquare root by squaring the result, and testing\r\nthat it is tolerably close to the original\r\nnumber. They might also check that some\r\ndata was successfully inserted into a database\r\nby then searching for it, and so on. Be cautious when the same person has\r\nwritten both the original routine and its\r\ninverse, as some bugs might be masked by\r\na common error in both routines. Where\r\npossible, use a different source for the\r\ninverse test. In the square root example,\r\nwe might use regular multiplication to test\r\nour method. For the database search, we\r\nwill probably use a vendor-provided search\r\nroutine to test our insertion."
    },
    {
      "tag": "div",
      "text": "Cross-Check Using Other Means"
    },
    {
      "tag": "p",
      "text": "Developers might also be able to crosscheck\r\nresults of their method using different\r\nmeans. Usually there is more than one\r\nway to calculate some quantity; we might\r\npick one algorithm over the others because\r\nit performs better or has other desirable\r\ncharacteristics. That is the one we will use\r\nin production, but we can use one of the\r\nother versions to cross-check our results in\r\nthe test system. This technique is especially\r\nhelpful when there is a proven, known\r\nway of accomplishing the task that happens\r\nto be too slow or too inflexible to use\r\nin production code. Another way of looking at this is to use\r\ndifferent pieces of data from the code\r\nitself to make sure they all add up . For\r\ninstance, suppose you had some sort of\r\nsystem that automated a lending library. In\r\nthis system, the number of copies of a\r\nparticular book should always balance.\r\nThat is, the number of copies that are\r\nchecked out plus the number of copies sitting\r\non the shelves should always equal the\r\ntotal number of copies in the collection.\r\nThese are separate pieces of data, and may\r\neven be managed by different pieces of\r\ncode, but they still have to agree and so can\r\nbe used to cross-check one another."
    },
    {
      "tag": "div",
      "text": "Force Error Conditions"
    },
    {
      "tag": "p",
      "text": "In the real world, errors happen. Disks fill\r\nup, network lines drop, e-mail goes into a\r\nblack hole, and programs crash. You\r\nshould be able to test that code handles all\r\nof these real-world problems by forcing\r\nerrors to occur. That is easy enough to do with invalid\r\nparameters and the like, but to simulate\r\nspecific network errors  without unplugging\r\nany cables  takes some special techniques,\r\nincluding using mock objects. In movie and television production,\r\ncrews will often use stand-ins , or doubles,\r\nfor the real actors. In particular, while the\r\ncrews are setting up the lights and camera\r\nangles, they will use lighting doubles : inexpensive,\r\nunimportant people who are\r\nabout the same height and complexion as\r\nthe very expensive, important actors who\r\nremain safely lounging in their luxurious\r\ntrailers. The crew then tests their setup with\r\nthe lighting doubles, measuring the distance\r\nfrom the camera to the stand-in's\r\nnose, adjusting the lighting until there are\r\nno unwanted shadows, and so on, while\r\nthe obedient stand-in just stands there and\r\ndoes not whine or complain about lacking\r\nmotivation for their character in this scene. What you can do in unit testing is similar\r\nto the use of lighting doubles in the\r\nmovies: Use a cheap stand-in that is kind\r\nof close to the real thing, at least superficially,\r\nbut that will be easier to work with\r\nfor your purposes."
    },
    {
      "tag": "div",
      "text": "Performance Characteristics"
    },
    {
      "tag": "p",
      "text": "One area that might prove beneficial to\r\nexamine is performance characteristics \r\nnot performance itself, but trends as input\r\nsizes grow, as problems become more\r\ncomplex, and so on. Why? We have all\r\nexperienced applications that work fine for\r\na year or so, but suddenly and inexplicably\r\nslow to a crawl. Often, this is the result of\r\na silly error or oversight: Adatabase\r\nadministrator changed the indexing structure\r\nin the database, or a developer typed\r\nan extra zero into a loop counter. What we would like to achieve is a\r\nquick regression test of performance\r\ncharacteristics. We want to do this regularly,\r\nevery day at least, so that if we have\r\ninadvertently introduced a performance\r\nproblem we will know about it sooner\r\nrather than later (because the nearer in\r\ntime you are to the change that introduced\r\nthe problem, the easier it is to work\r\nthrough the list of things that may have\r\ncaused that problem). So, to avoid shipping software with\r\nunsuspected performance problems,\r\nteams should consider writing some rough\r\ntests just to make sure that the performance\r\ncurve remains stable. For instance,\r\nsuppose the team is working on a component\r\nthat lets users browse the Web from\r\nwithin their application. Part of the\r\nrequirement is to filter out access to Web\r\nsites that we wish to block. The code\r\nworks fine with a few dozen sample sites,\r\nbut will it work as well with 10,000?\r\n100,000? We can write a unit test to find\r\nout. This gives us some assurance that we\r\nare still meeting performance targets. But\r\nbecause this one test takes six to seven\r\nseconds to run, we may not want to run it\r\nevery time. As long as we run it (say)\r\nnightly, we will quickly be alerted to any\r\nproblems we may introduce while there is\r\nstill time to fix them."
    },
    {
      "tag": "div",
      "text": "Getting Started"
    },
    {
      "tag": "p",
      "text": "All of the software tools mentioned in this\r\narticle are freely available on the Web. To\r\nget started using these practices effectively,\r\nwe recommend following this sequence: Get everything into version control. Arrange for automatic, daily builds.\r\nIncrease these to multiple times per day\r\nor continuously as soon as the process\r\nbegins to work smoothly. Start writing unit tests for new code.\r\nWhere needed, add some unit tests to\r\nexisting code (but be pragmatic about\r\nit; only add tests if they will really help,\r\nnot just for the sake of completeness). Add the unit tests to the scheduled\r\nbuilds. You can begin right away. Fire up that\r\nWeb browser and start downloading some\r\nsoftware if you do not already have it.\r\nThese ideas will not fix all the problems\r\non your project, of course, but they will\r\nprovide your project with a firm footing\r\nso you can concentrate on the truly difficult\r\nproblems."
    },
    {
      "tag": "div",
      "text": "References"
    },
    {
      "tag": "p",
      "text": "Zeichick, Alan. \"Debuggers, Source\r\nControl Keys to Quality.\" Software\r\nDevelopment Times 1 Mar. 2002. Cusumano, Michael, et al. \"A Global\r\nSurvey of Software Development\r\nPractices.\" Paper 178. MIT Sloan\r\nSchool of Management, June 2003. Thomas, Dave, and Andy Hunt. Pragmatic Version Control With CVS .\r\nRaleigh, NC: Pragmatic Bookshelf,\r\n2003 www.PragmaticBookshelf.com . Clark, Mike. Pragmatic Project\r\nAutomation. Raleigh, NC: Pragmatic\r\nBookshelf, 2004 www.PragmaticBookshelf.com . Hunt, Andy, and Dave Thomas. Pragmatic Unit Testing in Java With\r\nJUnit. Raleigh, NC: Pragmatic\r\nBookshelf, 2003. (Also available in a\r\nC# version) www.PragmaticBookshelf.com ."
    },
    {
      "tag": "div",
      "text": "About the Authors"
    },
    {
      "tag": "p",
      "text": "Andy Hunt is an avid\r\nwoodworker and musician,\r\nbut curiously, he is\r\nmore in demand as a\r\nconsultant. He has worked\r\nin telecommunications,\r\nbanking, financial services, and\r\nutilities, as well as more exotic fields such\r\nas medical imaging and graphic arts.\r\nHunt is author of many articles,\r\ncolumns and books, and co-author of\r\n\"The Pragmatic Programmer.\"\r\nThe Pragmatic Programmers,LLC 9650 Strickland RD STE 103-255 Raleigh, NC 27615 Phone: (800) 699-7764 E-mail: andy@pragmaticprogrammer.com Dave Thomas likes to\r\nfly single-engine airplanes\r\nand pays for his\r\nhabit by finding elegant\r\nsolutions to difficult\r\nproblems, consulting in\r\nareas as diverse as aerospace, banking,\r\nfinancial services, telecommunications,\r\ntravel and transport, and the Internet.\r\nThomas is author of many articles,\r\ncolumns and books, and co-author of\r\n\"The Pragmatic Programmer.\"\r\nThe Pragmatic Programmers,LLC P.O. Box 293325 Lewisville,TX 75029 Phone: (972) 539-7832 E-mail: dave@pragmaticprogrammer.com"
    },
    {
      "tag": "div",
      "text": "2004 The Pragmatic Programmers, LLC. Portions of\r\nthis article adapted from \"Pragmatic Unit Testing in Java\r\nWith JUnit,\" by Andy Hunt and Dave Thomas (Volume\r\nII of the Pragmatic Starter Kit), published by the\r\nPragmatic Bookshelf and Copyright  2003, 2004 The\r\nPragmatic Programmers, LLC. Reprinted with permission. Privacy and Security Notice External Links Disclaimer Site Map Contact Us Please E-mail or call 801-775-5555 (DSN 775-5555) if you have any questions regarding your CrossTalk subscription or for additional STSC information. Webmaster: 517th SMXS/MDEA, 801-777-0857 (DSN 777-0857), E-mail STSC Parent Organizations: 309SMXG Ogden Air Logistics Center, Hill AFB"
    }
  ]
}