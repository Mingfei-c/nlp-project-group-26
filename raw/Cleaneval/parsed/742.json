{
  "url": "http://www.testing.com/writings/status/status.html",
  "file_number": "742",
  "content_hash": "6d003b432e34b34e8ca3a5c3129264fa42eb132cd4af5f2f9f948fc616ffeed0",
  "total_blocks": 204,
  "retained_blocks": 29,
  "ignored_blocks": 175,
  "blocks": [
    {
      "tag": "p",
      "text": "testing.com > Writings > Test Manager at the\r\nStatus Meeting Testing Foundations Consulting in Software Testing Brian Marick Services Writings Tools Agile Testing The\r\nTest Manager at the Project Status Meeting Brian Marick, Testing Foundations ( marick@testing.com ) Steve Stukenborg, Pure\r\nAtria (now Rational ) Copyright 1997 by\r\nBrian Marick and Pure Atria Corporation. All Rights Reserved. A PDF version A discussion Project management must decide when a product is ready to\r\nrelease to end users. That decision is based on a mass of\r\ninevitably imperfect information, including an estimate of the\r\nproduct's bugginess. The testing team creates that estimate. The\r\ntest manager reports it, together with an assessment of how\r\naccurate it is. If it's not accurate enough, because not enough\r\ntesting has been done, the test manager must also report on what\r\nthe testing team is doing to obtain better information. When will\r\nproject management know enough to make a reasoned ship decision? The test manager has two responsibilities: to report the right information, and to report the information right . Both are\r\nequally important. If you don't have the right information, you're\r\nnot doing your job. If you don't report it so that it's accepted,\r\nvalued, and acted upon, you might as well not be doing your job. To address both topics, this paper is organized around the\r\nsituation in which the test manager presents \"the public\r\nface\" of the testing team: the status meeting. It covers the\r\ninformation to be presented and the needs of the people involved\r\n(which govern how to present that information usefully). 1. Context We assume a product developed in two distinct phases: feature\r\ndevelopment and product stabilization. We are specifically\r\nconcerned with status meetings during stabilization, which is\r\nwhen the pressure is highest. During the stabilization phase,\r\ndevelopers should do nothing but fix bugs. Ideally, they do not\r\nadd or change features. Testers do three types of testing during stabilization:"
    },
    {
      "tag": "div",
      "text": "Planned testing. The tester is assigned some\r\n        functional area or property of the product (such as the\r\n        \"printing subsystem\" or \"response to heavy\r\n        network load\"). At the beginning of the task, he\r\n        knows the approach to take, what a \"complete\"\r\n        set of tests would look like, and how much time is\r\n        allocated. (This knowledge, like anything else in the\r\n        project, is subject to revision, but it should be roughly\r\n        correct.) Guerrilla testing. This is testing that\r\n        opportunistically seeks to find severe bugs wherever they\r\n        may be. Guerrilla testing is much less planned than the\r\n        previous kind. The extent of planning might be: \"Jane,\r\n        you're the most experienced tester, and you have a knack\r\n        for finding bugs. From now until the end of the project,\r\n        bash away at whatever seem to be the shakiest parts of\r\n        the product.\" Guerrilla tests are usually not\r\n        documented or preserved. See [Kaner93] for more\r\n        information. Regression testing. The tester reruns tests to see\r\n        if one that used to pass now fails. If so, some change\r\n        has broken the product."
    },
    {
      "tag": "p",
      "text": "In the first part of stabilization, planned tests dominate. As\r\nstabilization proceeds, more and more regression testing is done.\r\nAt the end of the project, the testing effort shifts entirely to\r\nregression testing and guerrilla testing. For more about the stabilization phase, see [Cusumano96] and [Kaner93].\r\nFor various software development models, see [McConnell96]. 2. A Note on Numbers Throughout the paper, we ask you to compare actual results to\r\nyour plan. For example, we recommend that you track what\r\nproportion of their time your team spends rerunning tests vs. how\r\nmuch you expected they'd spend. A natural question is, \"Well,\r\nthen, what proportion should we expect?\" We're not\r\ngoing to answer that question because the answers vary and, to be\r\nuseful, require project-specific context. On the other hand, we\r\nthink it's terrible that test managers have nowhere to go to get\r\nthose answers, other than to buttonhole their colleagues and swap\r\nwar stories. For that reason, we will try to collect a variety of\r\nanswers and rules of thumb from experienced test managers and put\r\nthem on the web page http://www.stlabs.com/marick/root.htm. 3. Questions to\r\nAnswer in the Status Meeting During the project status meeting, you should be ready to\r\nanswer certain questions, both explicit and implicit. 3.1 When will we\r\nbe ready to ship? During stabilization, successive builds are made, found not to\r\nbe good enough, and replaced. It's a stressful time because,\r\nearly in stabilization, it often seems as if the product will never be good enough. There's a lot of pressure to show forward\r\nprogress. Much attention revolves around a graph like this: Everyone wants to know when those lines will cross, meaning\r\nall must-fix bugs have been fixed. Part of the answer comes from\r\nknowing how fast open bugs will be resolved. But another part of\r\nit comes from knowing how many new must-fix bugs can be expected.\r\nOur argument in this section is as follows:"
    },
    {
      "tag": "div",
      "text": "Estimating this number is extremely difficult. If you don't do it, someone else will, probably worse\r\n        than you would. So you should do it. Slight changes to the \"intuitive way\" of test\r\n        planning and tracking can make the estimates better. So\r\n        make those changes, as long as they don't get in the way\r\n        of finding bugs."
    },
    {
      "tag": "p",
      "text": "To estimate this number, the natural tendency is to do some\r\nsort of curve-fitting (by using a ruler, a spreadsheet, or some\r\nmore sophisticated tool). If you do, your extrapolations will\r\nlikely be wildly wrong, because that curve is due to a changing\r\nmixture of completed tasks, tasks in progress, and tasks not yet\r\nbegun, each with a different rate of finding bugs. Better than\r\nextrapolating from the summary graph is extrapolating from each\r\ntesting task individually, using a spreadsheet like this: Important What's being estimated with this spreadsheet is the total\r\nnumber of bugs this testing effort will find before it's\r\nfinished . It is not the total number of bugs in the product.\r\nIt is not the number of failures customers will see or report.\r\nThose would be better numbers to know, but this one is\r\nnevertheless useful. Comparing apples and oranges The spreadsheet above assumes that bugs will be found at\r\nroughly the same rate throughout a task. That's unlikely. A\r\ntypical graph of cumulative bugs discovered vs. working time (which\r\nis more meaningful than calendar time) will look like this: The reason for the curve is that different activities are\r\nlumped together in one task. Early in the task, the tester is\r\nlearning an area of the product and designing tests. While test\r\ndesign does find bugs, the rate of bug discovery is lower than\r\nwhen the tests are run for the first time. So, when test\r\nexecution begins, the curve turns up. As testing proceeds, an\r\nincreasing amount of time will be spent in regression testing (especially\r\nchecking bug fixes). Because rerunning a test is less likely to\r\nfind a bug than running it for the first time, the curve begins\r\nto flatten out. Your estimates will be more accurate if you track test design,\r\nfirst-time execution, and regression testing separately. Notes:"
    },
    {
      "tag": "div",
      "text": "We don't pretend that test execution doesn't have an\r\n        element of design in it - while running tests, testers\r\n        get ideas for new tests, which they then execute. But\r\n        counting that \"embedded design\" as test\r\n        execution should do no harm to what are already rough\r\n        estimates. Major discoveries should make you schedule\r\n        additional explicit tasks to be tracked separately. The beginning of test execution will probably find more\r\n        bugs than the end, because it's common for testers to run\r\n        the most promising tests first. Even if tests were run in\r\n        random order, some bugs are obvious enough that many of\r\n        the tests will find them. While this \"front-loading\"\r\n        of bugs might skew your estimates, you certainly don't\r\n        want to improve them by intentionally finding bugs later! You might find it most useful not to track retesting of\r\n        bug fixes. Instead, simply assume that some percentage of\r\n        fixed bugs will cause new bugs that will later be found\r\n        by retesting. Discover the percentage not by searching\r\n        the literature (the numbers vary widely) but by reference\r\n        to experience with your own project or other projects in\r\n        your company. In guerrilla testing, in which there's no formal design\r\n        nor much rerunning of tests, the bug discovery rate\r\n        nevertheless usually follows an S-shaped pattern, if not\r\n        a more sawtooth pattern as the tester switches from\r\n        opportunity to opportunity:"
    },
    {
      "tag": "p",
      "text": "A linear estimate early in the effort would guess high.\r\n    That's not necessarily bad - not because it gives you more\r\n    breathing room, but because the harm of estimating high is\r\n    less than the harm of estimating low. You should temper the\r\n    estimate with some judgment about when the curve is likely to\r\n    level off. Reality Check You may object that this approach seems hopelessly idealistic.\r\nYour project is chaotic enough, with code that changes so much,\r\nand so many bugs, and so much churn in the features, and... that\r\nthere's no hope of having tasks that stay the same long enough\r\nfor extrapolations to be meaningful. If that's the case - if your\r\nstabilization phase is anything but stable and you're really\r\ndoing mostly guerrilla testing - there's no point in presenting\r\nwildly inaccurate estimates at the project status meeting.\r\nConcentrate on answering the other questions presented in this\r\npaper. What about unstarted tasks? Here's a different spreadsheet, one for a project in which\r\nconfiguration testing hasn't started yet: If you haven't started a task, you have no data from which to\r\nextrapolate. But you have to, because the project manager needs\r\nestimates in order to run the project. Here are some solutions:"
    },
    {
      "tag": "div",
      "text": "Don't get into this situation. By the time the\r\n        stabilization phase is well enough underway that this\r\n        graph becomes the focus of attention, you should have\r\n        made some progress on all testing tasks. That almost\r\n        certainly means some context switching. For example, the\r\n        tester responsible for the scenario testing task will\r\n        have to stop part-way through and do some configuration\r\n        testing. That's annoying, disruptive, and reduces\r\n        efficiency - but only if you take a tester-centric view\r\n        of the universe, one where getting testing done is the\r\n        primary goal. Your team's primary goal is discovering\r\n        useful information and making it available as early as\r\n        possible. The project manager needs to know if there are\r\n        configuration problems before the last minute. Extrapolate using numbers for testing that's already\r\n        underway. For example, both \"test printing\" and\r\n        \"test editing\" are functional tests of discrete\r\n        subsystems. If the printing subsystem is half the size of\r\n        the editing subsystem, and half as much testing is\r\n        planned, you might reasonable predict half as many fatal\r\n        bugs will be discovered. There are, of course, many\r\n        variables that might confound that estimate: the\r\n        programmer who wrote the printing code may be a much\r\n        better programmer, the size metric you used (be it lines\r\n        of code, number of branches, function points, programmer\r\n        hours spent) may not capture the relative bugginess of\r\n        the two subsystems, and so on. However, it's a better\r\n        estimate than simply guessing, and it will improve\r\n        rapidly as real testing begins. Extrapolate using numbers from past projects. We\r\n        recommend that anyone doing any sort of data mining read\r\n        up on the statistical subfield of exploratory data\r\n        analysis. Two entertaining and influential early books\r\n        are [Tukey77] and [Mosteller77]."
    },
    {
      "tag": "p",
      "text": "Reality check: you will certainly be wrong Even if you are careful with your extrapolation, you should\r\nrealize that the number calculated is certainly wrong. Attaching\r\ntoo much certainty to the numbers - and especially striving for\r\nmeaningless precision - will lead to madness. Concentrate on\r\ntesting, and hope that the errors in your estimates roughly\r\ncancel each other out. Ideally, you'd like to give your estimates in the form of\r\nintervals, like this:"
    },
    {
      "tag": "div",
      "text": "Week 1 \"We\r\n        predict 122 to 242 bugs. Best estimate is 182.\" Week 2 \"We\r\n        predict 142 to 242 bugs. Best estimate is 192.\" Week 3 \"We\r\n        predict 173 to 233 bugs. Best estimate is 203.\" Week 4 \"We\r\n        predict 188 to 228 bugs. Best estimate is 208.\" Week 5 \"We\r\n        predict 205 to 215 bugs. Best estimate is 210.\""
    },
    {
      "tag": "p",
      "text": "Unfortunately, we expect your data will be unstable enough\r\nthat your calculated \"confidence intervals\" will be so\r\nwide as to be pointless. We're also unaware of statistical\r\ntechniques that apply to this particular problem. If you wish to\r\nexplore related notions, see the large body of literature on\r\nsoftware reliability and software reliability growth ([Musa87][Lyu96]).\r\nYour best bet might be using historical project data. If your\r\ncompany keeps good records of past projects, and has a reasonably\r\nconsistent process, you may be able to use errors in historical\r\nestimates to make predictions for this project. We think that, rather than straining for numbers far more\r\nexact than anything else in the project, you should concentrate\r\non understanding and explaining what caused any recent changes in\r\nthe estimates. Here, your goal is confidence - in you and your\r\nteam - not confidence intervals. You want to say things like:"
    },
    {
      "tag": "div",
      "text": "\"Last week's prediction was roughly 69 more must-fix\r\n        bugs. However, Dawn noticed some oddities as she\r\n        continued the testing of the modules from our outsource\r\n        partners, NightFly Technologies. She beefed up her test\r\n        plan with some targeted use case tests. That led to last\r\n        week's jump in bugs found, and it raised our prediction\r\n        to roughly 95 bugs.\""
    },
    {
      "tag": "p",
      "text": "The description behind the numbers is much more useful\r\nthan the numbers, because it helps the project manager make\r\ncontingency plans. Final notes on estimation"
    },
    {
      "tag": "div",
      "text": "Toward the end of the stabilization phase, your estimates\r\n        will become useless. That's the point at which your team\r\n        is concentrating on regression tests for the last few bug\r\n        fixes, together with guerrilla testing that tries to\r\n        squeeze out a few more must-fix bugs. This period is\r\n        inherently unpredictable - a single lingering bug could\r\n        take a month to fix. It's the worst part of the project.\r\n        You will still have provided a valuable service by\r\n        helping to predict when the hellish period starts. [DeMarco82] has useful things to say about estimation. [Bach94]\r\n        describes a bug tracking system initially used to produce\r\n        estimates of the sort described here. See also the\r\n        description of Bach's work in [Keuffel94], which says,\r\n        \"Bach now believes too many independent variables\r\n        exist to make useful predictions of ship dates by simply\r\n        leveraging off the critical-bug database.\" [Keuffel95]\r\n        continues the description with some interesting\r\n        deductions you can make from the shape of a project-wide\r\n        graph. Make sure that the graph of found vs. fixed \"must\r\n        fix\" bugs doesn't dominate attention to the\r\n        exclusion of other shipping criteria. Other criteria that\r\n        are sometimes used include trends in number of bugs found,\r\n        active, fixed, and verified; number of lower-severity\r\n        unfixed bugs; changes in bug severity distribution; and\r\n        amount of recent change to the source code ([Cusumano95],\r\n        [Kaner93], [Rothman96]). And the product shouldn't ship\r\n        until testing is finished, including regression testing\r\n        and a last-chance guerrilla test of the final build."
    },
    {
      "tag": "p",
      "text": "3.2 Which bugs\r\nshould be fixed? A discussion of individual bugs might happen in the project\r\nmeeting or in a separate bug classification meeting. (This might\r\nbe a discussion of all bugs or, more productively, a discussion\r\nof only controversial bugs.) You are likely to be the person who\r\nprints and distributes the summaries and detailed listings of new\r\nand unresolved bugs. It's preferable if the project manager leads\r\nthe meeting, not you. First, he or she has the ultimate\r\nresponsibility. Second, you will be an active participant, and it's\r\ndifficult to do a good job as both participant and meeting leader. Here are some key points:"
    },
    {
      "tag": "div",
      "text": "Developers have a psychological interest in assigning\r\n        bugs a low priority or deferring them. However, you may\r\n        well tend to over-emphasize the severity of a bug, which\r\n        is just as harmful. The best advocates for fixing a bug\r\n        are the people who will have to live with its\r\n        consequences. That's not you. It's more likely to be\r\n        customer service people and marketing people. They should\r\n        be invited to bug classification meetings. Your role as\r\n        advocate is to make sure that the bug report is\r\n        understood correctly. If the bug report is being\r\n        misinterpreted (for example, as being an unlikely special\r\n        case instead of a symptom of a more general problem),\r\n        correct the misinterpretation. The key issue in such a meeting is determining whether\r\n        the risk of fixing a bug exceeds the risk of shipping it\r\n        with the product. When helping assess risk, your job is\r\n        to provide two bits of informed opinion:"
    },
    {
      "tag": "blockquote",
      "text": "What are the testing implications of fixing the bug (whether\r\n            it's fixed alone or batched with other bugs in the\r\n            same subsystem)? How much will adequately testing the\r\n            fix cost? Do you have the resources to do it? Is it a\r\n            matter of rerunning regression tests? (What's the\r\n            backlog?) Or will new tests have to be written? What's the rate of regressions? How often do bug\r\n            fixes fail? (If the rate of regressions is high,\r\n            especially in the subsystem in question, and that\r\n            subsystem has light test coverage, the risk of a\r\n            bugfix is high.)"
    },
    {
      "tag": "div",
      "text": "Higher management may be tracking open bug counts. That\r\n        can lead to unproductive pressure to make the numbers\r\n        look good at the expense of fixing fewer but more\r\n        important bugs. (See [Kaner93], chapter six.) As a\r\n        responsible project member, you should be alert to\r\n        decisions that provide only internal benefit, not benefit\r\n        to the customer."
    },
    {
      "tag": "p",
      "text": "We urge you to read the discussions of bugs and bug reporting\r\nin [Bach94] and [Kaner93]. 3.3 Where are the\r\ndanger areas in the project? As part of the estimation process, you will become aware of\r\n\"hot spots\" in the product: areas where there are\r\nunusually many bugs. In the charts above, testing editing is\r\nfinding far more bugs than any other testing task. The project manager is sometimes the last to know about hot\r\nspots, since developers are incorrigibly optimistic. They tend\r\nnot to admit (to themselves) that a particular subsystem is out\r\nof control until too late. It's your job as a test manager to\r\nreport on hot spots at the status meeting. You need to do this\r\nwith care, lest your team be viewed as the enemy by development.\r\nHere are some tips."
    },
    {
      "tag": "div",
      "text": "Make sure it really is a hot spot."
    },
    {
      "tag": "p",
      "text": "The heat of a hot spot is relative to the size and\r\n    complexity of the area under test. If the editing subsystem\r\n    is four times as complex as the printing subsystem, four\r\n    times as many bugs is not surprising. You should have\r\n    complexity information available, because you should have\r\n    used it in test planning. Historical information will help to keep hot spots in\r\n    perspective. What's the bug rate for previous projects (per\r\n    line of code, function point, or some other measure of size)?\r\n    What parts of this project are out of line with the\r\n    historical averages?"
    },
    {
      "tag": "div",
      "text": "Report on product, not people."
    },
    {
      "tag": "p",
      "text": "With perhaps a few exceptions, such as configuration\r\n    testing, the hot spots you find will be associated with a\r\n    particular person or team. Don't criticize them. Maybe they've\r\n    done a shoddy job, but don't assume you have enough\r\n    information to make that judgement. Even if you do, it's\r\n    counterproductive at this point. Not only must you not criticize the developers, it's\r\n    important that they not think you're criticizing them. Your\r\n    manner should be calm, dispassionate, and professional. Use\r\n    \"we\" instead of \"you\". You should take\r\n    care to say something good along with the bad news: \"That last overhaul of the account management\r\n        module has really streamlined the user interface. Paul,\r\n        the tester, says that it takes half as many mouse moves\r\n        to get through a test as before. Of course, that means\r\n        that users will see the same speedup. Unfortunately, his\r\n        tests have found a big jump in the number of bugs, to the\r\n        point where we're well above average for modules in this\r\n        product.\""
    },
    {
      "tag": "div",
      "text": "Warn of hot spots in advance."
    },
    {
      "tag": "p",
      "text": "Do not announce a hot spot without prior warning. You know\r\n    you've damaged your usefulness when the project manager\r\n    explodes, \"What do you mean, the networking subsystem is\r\n    in crisis! Why haven't I heard about this before?!\" One reason for no prior warning is that there's no data.\r\n    Some testing task hadn't started until the past week. When it\r\n    did start, the tester involved found so many bugs she had to\r\n    down tools right away. Avoid the issue by doing at least a\r\n    little testing on all tasks as soon as possible. You can\r\n    defuse the issue by reporting, at each status meeting, which\r\n    important tasks haven't started, along with a prediction of\r\n    when each will start. Another reason for a sudden announcement is that a testing\r\n    task suddenly crosses the threshold where you feel you should\r\n    raise the alarm. Avoid this by announcing which tasks you're\r\n    watching more carefully, though they don't yet merit the\r\n    designation \"hot spot\"."
    },
    {
      "tag": "div",
      "text": "Track hot spots carefully."
    },
    {
      "tag": "p",
      "text": "The response to a hot spot should be some development\r\n    effort (redesign or rework). When that's complete, report on\r\n    the success: \"We reran all the regression tests on the\r\n    new build, with unusually few regressions, which personally\r\n    gives me a warm and fuzzy feeling. We've started executing\r\n    new tests on the module, and things look good so far - many\r\n    fewer bugs than in the last version. I think we're out of the\r\n    woods on this one.\" An alternative to rework is to \"debug the code into\r\n    working\". In that case, you need to adjust your plans.\r\n    Whereas before you scheduled a fixed amount of testing, you\r\n    now have an open-ended testing task. You might increase the\r\n    number of designed tests; you will certainly devote more time\r\n    to guerrilla testing. You need to continue testing until you\r\n    stop discovering a disproportionately large number of bugs.\r\n    You will look for a graph like this: After nearly twice as much testing as planned, you seem to\r\n    have exhausted the bugs in the code. Another type of hot spot In addition to total bugs, track \"churn\": how many\r\nregression tests fail. If a disproportionately high number of bug\r\nfixes or other changes break what used to work, there's a problem.\r\nThe subsystem is likely fragile and deserves rework or a greater\r\ntesting effort. Here's a chart that might cause alarm: 3.4 Are you\r\nworking smart? Any buggy area announced late in the project is going to be\r\nstressful. It will likely mean some upheaval, some shifting of\r\ndevelopment priorities. As test manager, you must also shift your\r\npriorities, both because you now know more about risky areas and\r\nalso because you must be seen as doing more than smugly\r\nannouncing a crisis and letting someone else deal with it. It's\r\nimportant to both be helpful and also be seen to be helpful. Display graphs to show that the testing team is flexibly\r\nadapting what you do to what you discover. One such graph follows.\r\nIt breaks testing down by task. It plots effectiveness (bugs\r\nfound) behind measures of effort to date. That way, mismatches\r\nbetween effort and result are visually striking. The graph uses\r\ntwo measures of effort: time spent and coverage achieved.\r\nCode coverage measures how thoroughly tests exercise the code.\r\nThat correlates roughly with thoroughness of testing. The most\r\ncommon type of code coverage measures which lines of code have\r\nbeen executed. For other types of coverage, see [Kaner96] and [Marick95].\r\nFor a discussion of how coverage can be misused, see [Marick97]. Here's what you might say in the project status meeting. (Notice\r\nthat the individual testing activities tracked separately for bug\r\ncount estimation are here grouped into larger tasks.) \"The stress tests and db tests are paying off in terms of\r\nbugs found. The stress tests are only 14% complete, so you can\r\nexpect a lot of stress-type bugs in the next few weeks. You'll\r\nnotice that we early in the project cut back on the number of\r\nstress tests - although we're 14% complete against plan, that's\r\nonly three percent of what we originally hoped to do. If\r\nstress testing keeps finding bugs, we'll up the amount of stress\r\ntesting planned. \"The db tests are 55% complete against plan, and they've\r\ngot 80% coverage. Given the type of testing we're doing, we might\r\nsee the bug finding rate start leveling off soon. We may not do\r\nall the testing planned. \"The GUI tests have been a washout. We're not finding\r\nbugs. The code seems solid. We're stopping that testing now and\r\nshifting the tester onto network testing, where we're finding\r\ntons of bugs. In the network testing, you'll note that the\r\ncoverage is high in comparison to the number of tests written.\r\nThat's because we've finished the \"normal use tests\".\r\nThe error handling tests, which we couldn't start until the\r\nnetwork simulator was ready, will add less coverage, but we\r\nexpect them to be a good source of bugs. \"The calc tests were a botch. We're nearly done, without\r\nmuch to show for it. My fault - I let the schedule get away from\r\nme. We're stopping testing right away.\" Comparing yourself to others It's a common developer prejudice that testers write a lot of\r\ntests, but it's wasted motion because they're not finding the\r\nimportant bugs. First, the test manager needs to check whether\r\nthe prejudice is justified. (All too often, it is.) Second, if it's\r\nnot, reassure the project team. Doing both things starts with a\r\ngraph like this, which shows what percentage of bugs are being\r\nfound by different types of planned testing vs. those found by\r\nchance. This shows a fairly reasonable breakdown. Most new bugs are\r\nfound by the testing team: newly written feature tests,\r\nregression tests run again, guerrilla tests, and planned\r\nconfiguration tests. 10% of new bugs reported are due to\r\nregressions, which is not outlandish. Beta customers are\r\nreporting configuration bugs at a good clip, which is what you\r\nexpect from them. (You might want to consider doing more in-house\r\nconfiguration testing if the total number of configuration bugs\r\nis unusually high.) There are a reasonable number of bugs from\r\nother sources (beta bugs that are not configuration problems,\r\nbugs found by developers and other in-house use, and so on). This graph is more alarming: Too many bugs are being found by \"other\", not enough\r\nby the testing team. They're being discovered by chance. Can the\r\ntesting effort change to detect them more reliably? The fact that\r\nguerrilla testing is doing so much better than the new feature\r\ntests is also evidence that something's wrong with the planned\r\ntesting effort. Perhaps the problem is that the project is \"churning\":\r\nthere are too many regressions. Maybe almost everyone on the\r\ntesting team is spending so much time running regression tests\r\nthat they have little time to execute new ones (except for a few\r\nvery productive guerrilla testers). When many bugs have been found, the testing team can become a\r\nbottleneck. Report clearly when regression testing load exceeds\r\nthe plan. Such reports, made early and forthrightly, leave the\r\nimpression that slips in the testing schedule are a result of\r\npitching in to help the project. Done late, you may get the blame\r\nfor the final slips, slips that happen as developers wait for you\r\nto finish up your testing. When reporting on your schedule, remind everyone that the\r\nearliest possible ship date is the time required to complete all\r\nplanned tests, plus the time required to rerun all repeatable\r\ntests (both manual and automated) on the final build, and time\r\nfor a series of guerrilla tests on that build. 3.5 What does your\r\nwork mean? We've seen how you can predict the number of bugs testing will\r\nfind and use graphs to reallocate the testing effort\r\nappropriately. But what does all this effort mean? Recall that the project manager wants to reduce uncertainty\r\nand risk. He worries that this product will be an embarrassment\r\nor worse after it ships. Knowing how many bugs testing will find\r\nis useful, because it lets him predict whether the product can\r\nship on time, but what he really wants to know is how many\r\nbugs testing won't find. Let's assume that you graph the bugs found by new (not\r\nrerun) designed tests for some particular testing task. If that\r\ntask is consistent throughout (you don't apply different testing\r\ntechniques during different parts of the task, or save the\r\ntrickiest tests for the end), you'd expect a graph that looks\r\nsomething like this: You'd expect something of a flurry of new bugs at first. Many\r\nof those are the bugs that are easy to find, so that many\r\ndifferent tests would hit them. After those have been disposed of,\r\nthere's a more-or-less steady stream of new bugs. (In practice,\r\nthe curve might tail down more sharply, since there's a tendency\r\nto concentrate on the most productive tests first, in order to\r\nget bugs to the developers quickly.) This graph doesn't really let you predict what happens in the\r\nfield. You can't simply extrapolate the curve. If you did a\r\nperfect job of test design, you've tried all the bug-revealing\r\ncases, and all bugs have been found. If your test cases are very\r\nunrepresentative of typical customer use, you might have missed\r\nmany of the bugs customers will find, in which case there will be\r\na surge of new bug reports when the product is released (not\r\nunusual). So what can you do? You have to extrapolate from previous\r\nprojects."
    },
    {
      "tag": "div",
      "text": "If you discovered 70% of the configuration bugs during\r\n        test in release 3.0, and if you continue to perform\r\n        configuration testing roughly the same way, the safest\r\n        bet is that you'll again miss 30% of the bugs. If you consistently apply the same set of testing\r\n        techniques, and you size the testing effort against the\r\n        product in a consistent way, you should hope that you'll\r\n        find the same proportion of bugs from effort to effort.\r\n        In practice, of course, the variability among testers\r\n        \"doing the same thing\" is as large as it is for\r\n        programmers (that is to say, enormous). You can reduce\r\n        variability by having testers review each other's test\r\n        designs, but you won't eliminate it. That's OK. Saying\r\n        \"based on the past three projects, we predict\r\n        between 50 and 120 more must-fix bugs will be found by\r\n        customers\" beats only being able to say \"We\r\n        tested hard, and we hope we didn't miss too much.\""
    },
    {
      "tag": "p",
      "text": "When reporting extrapolations, don't give raw numbers. Say,\r\n\"we predict the reliability of this product will be typical\r\n[or high, or low] for this company.\" If possible, make\r\ncomparisons to specific products: \"This product did about\r\naverage on the planned tests, but remarkably few bugs were found\r\nin guerrilla testing or regression testing. The last product with\r\nthat pattern was FuzzBuster 2.0, which did quite well in the\r\nfield. We predict the same for this one.\" The accuracy of your predictions depends on a consistent\r\nprocess. Consistent processes are the subject of industrial\r\nquality control, especially the work of Shewhart, Deming, Juran,\r\nand others. We in software can learn much from them, so long as\r\nwe're careful not to draw unthinking analogies. [Deming82] is\r\noften cited, but the style is somewhat telegraphic. [Aguayo91]\r\nhas been recommended to one of us (Marick), who first learned\r\nabout statistical quality control from a draft of [Devor91]. The\r\nSoftware Engineering Institute's Capability Maturity Model can be\r\nthought of as an adaptation of modern industrial quality control\r\nto software, though the coverage of testing is sketchy. See [Humphry89]\r\nor [Curtis95]. 4. Summary of the\r\npaper As the frequent bearer of bad news, the test manager has a\r\ndifficult job. The job is easier if he or she appears competent,\r\nknowledgeable, and proactive. It's a matter of having the\r\nrelevant information, presenting it well, being able to answer\r\nreasonable questions. A goal should be to walk out of the status\r\nmeeting knowing that the testing team and its work have been\r\nrepresented well. As the test manager, you are the keeper of data that can help\r\nyou understand trends and special occurrences in the project,\r\nprovided you avoid the two \"data traps\": having\r\nunjustified faith in numbers, and rejecting numbers completely\r\nbecause they're imperfect. Take a balanced view and use data as a\r\nspringboard to understanding. 5. Acknowledgements Keith Davis made helpful comments on an earlier draft of this\r\npaper. 6. References [Aguayo91] Rafael Aguayo, Dr. Deming, Fireside / Simon and Schuster,\r\n1991. [Bach94] James Bach, \"Process Evolution in a Mad World,\" in Proceedings\r\nof the Seventh International Quality Week , (Software Research,\r\nSan Francisco, CA), 1994. [Curtis95] Mark C. Paulk, Charles V. Weber, and Bill Curtis (ed), The\r\nCapability Maturity Model: Guidelines for Improving the Software\r\nProcess , Addison-Wesley, 1995. [Cusumano95] M. Cusumano and R. Selby, Microsoft Secrets, Free Press,\r\n1995. [DeMarco82] Tom DeMarco, Controlling Software Projects: Management,\r\nMeasurement, and Estimation , Prentice Hall, 1982. [Deming82] J. Edwards Deming, Out of the Crisis, MIT Center for\r\nAdvanced Engineering Study, 1982. [Devor92] Richard E. Devor, Tsong-How Chang, and John W. Sutherland ,\r\nStatistical Quality Design and Control: Contemporary Concepts and\r\nMethods , MacMillan, 1992. [Humphrey89] Watts Humphrey, Managing the Software Process, Addison-Wesley,\r\n1989. [Kaner93] C. Kaner, J. Falk, and H.Q. Nguyen, Testing Computer Software\r\n(2/e), Van Nostrand Reinhold, 1993. [Kaner96] Cem Kaner, \" Software\r\nNegligence & Testing Coverage ,\" in Proceedings of\r\nSTAR 96 , (Software Quality Engineering, Jacksonville, FL),\r\n1996. () [Keuffel94] Warren Keuffel, \"James Bach: Making Metrics Fly at Borland,\" Software Development , December 1994. [Keuffel94] Warren Keuffel, \"Further Metrics Flights at Borland,\" Software\r\nDevelopment , January 1995. [Lyu96] Michael R. Lyu (ed.), Handbook of Software Reliability\r\nEngineering, McGraw-Hill, 1996. [Marick95] Brian Marick, The Craft of Software Testing, Prentice Hall,\r\n1995. [Marick97] Brian Marick, \" Classic\r\nTesting Mistakes ,\" in Proceedings of STAR 97 , (Software\r\nQuality Engineering, Jacksonville, FL), 1997. [McConnell96] Steve McConnell, Rapid Development, Microsoft Press, 1996. [Mosteller77] Frederick Mosteller and John W. Tukey, Data Analysis and\r\nRegression, Addison-Wesley, 1977. [Musa87] J. Musa, A. Iannino, and K. Okumoto, Software Reliability :\r\nMeasurement, Prediction, Application , McGraw-Hill, 1987. [Rothman96] Johanna Rothman, \" Measurements to\r\nReduce Risk in Product Ship Decisions ,\" in Proceedings\r\nof the Ninth International Quality Week , (Software Research,\r\nSan Francisco, CA), 1996. [Tukey77] John W. Tukey, Exploratory Data Analysis, Addison-Wesley,\r\n1977. Services Writings Tools Agile Testing Comments to marick@testing.com"
    }
  ]
}